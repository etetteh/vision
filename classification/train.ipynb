{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe094cf1-5432-434a-a7dd-ce36c317a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from functools import partial\n",
    "\n",
    "import timm\n",
    "import torch, torchvision\n",
    "\n",
    "from torch import nn\n",
    "from timm.loss import BinaryCrossEntropy, LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import autoaugment, transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "import transforms as T\n",
    "import utils\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining, pb2\n",
    "\n",
    "\n",
    "import simpleargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78eb60-2460-46a3-87f5-938dd1f3d270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fab6e6-5c28-48e4-adcb-f34c9997b410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f4252-186e-453d-8c0b-87926d14ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir -p datasets\n",
    "# ! wget -nc --no-check-certificate https://download.pytorch.org/tutorial/hymenoptera_data.zip -P datasets\n",
    "# ! unzip -u datasets/hymenoptera_data.zip -d datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6508d-a171-4323-8da4-cbe17b82fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {}\n",
    "\n",
    "# ######### Recipe 1 #########\n",
    "# ### lr optimization ###\n",
    "# if lr_optim:\n",
    "#     config[\"lr\"] = tune.qloguniform(1e-4, 1e-1, 3e-5)\n",
    "#     config[\"lr_warmup_decay\"] = tune.qloguniform(1e-5, 1e-3, 2e-6)\n",
    "#     config[\"lr_warmup_epochs\"] = tune.qrandint(3, 15, 3)\n",
    "#     config[\"batch_size\"] = tune.grid_search([16, 32, 64, 128])\n",
    "    \n",
    "# ######### Recipe 2 #########   \n",
    "# config[\"interpolate\"] = tune.choice([InterpolationMode.BICUBIC, InterpolationMode.BILINEAR, InterpolationMode.NEAREST])\n",
    "# if trivial:\n",
    "#     config[\"num_magnitude_bins\"] = tune.randint(15, 35)\n",
    "# if random:\n",
    "#     config[\"num_ops\"] = tune.randint(2, 5)\n",
    "#     config[\"magnitude\"] = tune.randint(5, 9)\n",
    "#     config[\"num_magnitude_bins\"] = tune.randint(15, 35)\n",
    "    \n",
    "# ######### Recipe 4 #########  \n",
    "# ### data aug ###\n",
    "# if random_erase:\n",
    "#     config[\"random_erase_prob\"] = tune.quniform(0.1, 0.3, 0.05)\n",
    "    \n",
    "# ######### Recipe 5 ######### \n",
    "# ### label smoothing\n",
    "# if label_smoothing:\n",
    "#     config[\"label_smoothing\"] = tune.choice([0.05, 0.1, 0.15])\n",
    "\n",
    "# ######### Recipe 6 #########\n",
    "# ### mixup ###\n",
    "# if mixup:\n",
    "#     config[\"mixup_alpha\"] = tune.quniform(0.1, 0.5, 0.1)\n",
    "    \n",
    "# ######### Recipe 7 #########\n",
    "# ### cutmix ###\n",
    "# if cutmix:\n",
    "#     config[\"cutmix_alpha\"] = tune.quniform(0.4, 1.0, 0.1)      \n",
    "    \n",
    "# ######### Recipe 8 #########\n",
    "# ### weight decay tuning ###\n",
    "# if wd_tune:\n",
    "#     config[\"weight_decay\"] = tune.qloguniform(1e-5, 1e-3, 2e-6)\n",
    "#     norm_weight_decay=0.0\n",
    "           \n",
    "# ######### Recipe 9 #########   \n",
    "# ### fixres ###\n",
    "# if fixres:\n",
    "#     config[\"train_size_crop\"] = tune.grid_search([176, 192, 208])    \n",
    "\n",
    "# ######### Recipe 10 #########\n",
    "# ### model ema ###\n",
    "# if ema:\n",
    "#     config[\"model_ema_steps\"] = tune.qrandint(15, 50, 5)\n",
    "#     config[\"model_ema_decay\"] = tune.uniform(0.99, 0.99998)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244ea7f-f464-4dbb-9b1f-d18cbb948c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.hub.load(\"facebookresearch/deit:main\", \"deit_\"+mtype+\"_patch16_224\", pretrained=False)\n",
    "# parameters = model.parameters()\n",
    "\n",
    "\n",
    "# ######### Recipe 1 #########\n",
    "# if lr_optim:\n",
    "#     optimizer = torch.optim.AdamW(parameters, lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "#     main_lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs - config[\"lr_warmup_epochs\"])\n",
    "#     warmup_lr_scheduler = LinearLR(optimizer, start_factor=config[\"lr_warmup_decay\"], total_iters=config[\"lr_warmup_epochs\"])\n",
    "#     lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[config[\"lr_warmup_epochs\"]])\n",
    "\n",
    "\n",
    "# augment = [transforms.RandomResizedCrop(crop_size=224, interpolation=config[\"interpolate\"]),\n",
    "#            transforms.RandomHorizontalFlip()]\n",
    "\n",
    "# ######### Recipe 2 #########           \n",
    "# if random:\n",
    "#     augment.append(autoaugment.RandAugment(num_ops=config[\"num_ops\"], magnitude=config[\"magnitude\"], num_magnitude_bins=config[\"num_magnitude_bins\"], interpolation=config[\"interpolate\"]))\n",
    "# if trivial:\n",
    "#     augment.append(autoaugment.TrivialAugmentWide(num_magnitude_bins=config[\"num_magnitude_bins\"], interpolation=config[\"interpolate\"]))\n",
    "\n",
    "# augment.extend([\n",
    "#         transforms.PILToTensor(),\n",
    "#         transforms.ConvertImageDtype(torch.float),\n",
    "#         transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "#                              std=(0.229, 0.224, 0.225)),\n",
    "#         ])  \n",
    "# ######### Recipe 4 #########\n",
    "# if random_erase:\n",
    "#     augment.append(transforms.RandomErasing(p=config[\"random_erase_prob\"])\n",
    "\n",
    "                   \n",
    "# ######### Recipe 5 #########\n",
    "# if mixup or cutmix:\n",
    "#     if bce:\n",
    "#         train_criterion = BinaryCrossEntropy(smoothing=0.0)\n",
    "#     else:\n",
    "#         train_criterion = SoftTargetCrossEntropy()\n",
    "# elif label_smoothing:\n",
    "#     if bce:\n",
    "#         train_criterion = BinaryCrossEntropy(smoothing=config[\"label_smoothing\"])\n",
    "#     else:\n",
    "#         train_criterion = LabelSmoothingCrossEntropy(smoothing=config[\"label_smoothing\"])\n",
    "# else:\n",
    "#     train_criterion = nn.CrossEntropyLoss()\n",
    "# train_criterion = train_criterion.to(device)\n",
    "\n",
    "# if bce:\n",
    "#     valid_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# else:\n",
    "#     valid_criterion = torch.nn.CrossEntropyLoss()\n",
    "# valid_criterion = valid_criterion.to(device)\n",
    " \n",
    "                   \n",
    "# mixup_transforms = []\n",
    "# ######### Recipe 6 #########\n",
    "# if mixup:\n",
    "#     mixup_transforms.append(T.RandomMixup(num_classes, p=1.0, alpha=config[\"mixup_alpha\"]))\n",
    "\n",
    "# ######### Recipe 7 #########    \n",
    "# if cutmix_alpha:\n",
    "#     mixup_transforms.append(T.RandomCutmix(num_classes, p=1.0, alpha=config[\"cutmix_alpha\"]))\n",
    "# if mixup_transforms:\n",
    "#     mixupcutmix = torchvision.transforms.RandomChoice(mixup_transforms)\n",
    "#     collate_fn = lambda batch: mixupcutmix(*default_collate(batch))\n",
    "      \n",
    "                   \n",
    "# ######### Recipe 8 #########                   \n",
    "# if wd_tune:\n",
    "#     param_groups = torchvision.ops._utils.split_normalization_params(model)\n",
    "#     wd_groups = [args.norm_weight_decay, weight_decay]\n",
    "#     parameters = [{\"params\": p, \"weight_decay\": w} for p, w in zip(param_groups, wd_groups) if p]\n",
    "\n",
    "# ######### Recipe 9 #########\n",
    "# if fixres:\n",
    "#     augment[0] = transforms.RandomResizedCrop(crop_size=config[\"train_size_crop\"], interpolation=optimal_interpolation)  \n",
    "                   \n",
    "\n",
    "# ######### Recipe 10 #########\n",
    "# ema = None\n",
    "# if ema:\n",
    "#     adjust = 1 * batch_size * config[\"model_ema_steps\"] / epochs\n",
    "#     alpha = 1.0 - config[\"model_ema_decay\"]\n",
    "#     alpha = min(1.0, alpha * adjust)\n",
    "#     model_ema = utils.ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c455645-bdbf-4034-87e5-c6277a25fb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c24b46-01dc-453c-bc1d-f38df620bbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efe637-b526-4027-ba52-1932eca9bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(config, data_dir='./hymenoptera_data'):\n",
    "    augment = [transforms.RandomResizedCrop(size=224, interpolation=InterpolationMode.BILINEAR),\n",
    "               transforms.RandomHorizontalFlip()]\n",
    "    ######### Recipe 2 #########           \n",
    "    if cfg.random:\n",
    "        augment.append(autoaugment.RandAugment(num_ops=config[\"num_ops\"], magnitude=config[\"magnitude\"], num_magnitude_bins=config[\"num_magnitude_bins\"], interpolation=config[\"interpolation\"]))\n",
    "    elif cfg.num_ops:\n",
    "        augment.append(autoaugment.RandAugment(num_ops=cfg.num_ops, magnitude=cfg.magnitude, num_magnitude_bins=cfg.num_magnitude_bins, interpolation=cfg.interpolation))\n",
    "        \n",
    "    if cfg.trivial:\n",
    "        augment.append(autoaugment.TrivialAugmentWide(num_magnitude_bins=config[\"num_magnitude_bins\"], interpolation=config[\"interpolation\"]))\n",
    "    elif cfg.num_magnitude_bins:\n",
    "        augment.append(autoaugment.TrivialAugmentWide(num_magnitude_bins=cfg.num_magnitude_bins, interpolation=cfg.interpolation))\n",
    "\n",
    "    augment.extend([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),])  \n",
    "    ######### Recipe 4 #########\n",
    "    if cfg.random_erase:\n",
    "        augment.append(transforms.RandomErasing(p=config[\"random_erase_prob\"]))\n",
    "    elif cfg.random_erase_prob:\n",
    "        augment.append(transforms.RandomErasing(p=cfg.random_erase_prob))\n",
    "    \n",
    "    ######### Recipe 9 #########\n",
    "    if cfg.fixres:\n",
    "        augment[0] = transforms.RandomResizedCrop(size=config[\"train_crop_size\"], interpolation=cfg.interpolate)  \n",
    "    if cfg.train_crop_size:\n",
    "        augment[0] = transforms.RandomResizedCrop(size=cfg.train_crop_size, interpolation=cfg.interpolate)  \n",
    "\n",
    "    augment = transforms.Compose(augment)\n",
    "    \n",
    "    valid_augment = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "                       \n",
    "    trainset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=augment)\n",
    "    validset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=valid_augment)\n",
    "    num_classes = len(trainset.classes)\n",
    "    return trainset, validset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a611b52-efde-40e7-b4c7-e727305594cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, checkpoint_dir=None, data_dir=None):\n",
    "    seed = 99\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    trainset, validset, num_classes = load_data(config, data_dir=data_dir)\n",
    "    \n",
    "    model = torch.hub.load(\"facebookresearch/deit:main\", \"deit_\"+cfg.model+\"_patch16_224\", pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    num_ftrs = model.head.in_features\n",
    "    model.head = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    ######### Recipe 10 #########\n",
    "    ema_model = None\n",
    "    if cfg.ema:\n",
    "        adjust = 1 * cfg.batch_size * config[\"model_ema_steps\"] / cfg.epochs\n",
    "        alpha = 1.0 - config[\"model_ema_decay\"]\n",
    "        alpha = min(1.0, alpha * adjust)\n",
    "        ema_model = utils.ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "    elif (cfg.model_ema_decay and cfg.model_ema_steps):\n",
    "        adjust = 1 * cfg.batch_size * cfg.model_ema_steps / cfg.epochs\n",
    "        alpha = 1.0 - cfg.model_ema_decay\n",
    "        alpha = min(1.0, alpha * adjust)\n",
    "        ema_model = utils.ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "        \n",
    "    \n",
    "    parameters = model.head.parameters()\n",
    "    \n",
    "    ######### Recipe 8 #########                   \n",
    "    if cfg.wd_tune:\n",
    "        norm_weight_decay=0.0\n",
    "        param_groups = utils.split_normalization_params(model)\n",
    "        wd_groups = [norm_weight_decay, config[\"weight_decay\"]]\n",
    "        parameters = [{\"params\": p, \"weight_decay\": w} for p, w in zip(param_groups, wd_groups) if p]\n",
    "    elif cfg.weight_decay:\n",
    "        norm_weight_decay=0.0\n",
    "        param_groups = utils.split_normalization_params(model)\n",
    "        wd_groups = [norm_weight_decay, cfg.weight_decay]\n",
    "        parameters = [{\"params\": p, \"weight_decay\": w} for p, w in zip(param_groups, wd_groups) if p]\n",
    "    \n",
    "    \n",
    "    ######### Recipe 1 #########\n",
    "    if cfg.lr_optim:\n",
    "        optimizer = torch.optim.AdamW(parameters, lr=config[\"lr\"])\n",
    "        main_lr_scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs - config[\"lr_warmup_epochs\"])\n",
    "        warmup_lr_scheduler = LinearLR(optimizer, start_factor=config[\"lr_warmup_decay\"], total_iters=config[\"lr_warmup_epochs\"])\n",
    "        lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[config[\"lr_warmup_epochs\"]])\n",
    "    elif cfg.lr:\n",
    "        optimizer = torch.optim.AdamW(parameters, lr=cfg.lr)\n",
    "        main_lr_scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs - cfg.lr_warmup_epochs)\n",
    "        warmup_lr_scheduler = LinearLR(optimizer, start_factor=cfg.lr_warmup_decay, total_iters=cfg.lr_warmup_epochs)\n",
    "        lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[cfg.lr_warmup_epochs])\n",
    "        \n",
    "    ######### Recipe 5 #########\n",
    "    if cfg.mixup or cfg.cutmix:\n",
    "        if cfg.bce:\n",
    "            train_criterion = BinaryCrossEntropy(smoothing=0.0)\n",
    "        else:\n",
    "            train_criterion = SoftTargetCrossEntropy()\n",
    "    elif cfg.label_smoothing:\n",
    "        if cfg.bce:\n",
    "            if cfg.smooth:\n",
    "                train_criterion = BinaryCrossEntropy(smoothing=cfg.smooth)\n",
    "            else:\n",
    "                train_criterion = BinaryCrossEntropy(smoothing=config[\"label_smoothing\"])\n",
    "        else:\n",
    "            if cfg.smooth:\n",
    "                train_criterion = LabelSmoothingCrossEntropy(smoothing=cfg.smooth)\n",
    "            else:\n",
    "                train_criterion = LabelSmoothingCrossEntropy(smoothing=config[\"label_smoothing\"])\n",
    "    else:\n",
    "        train_criterion = nn.CrossEntropyLoss()\n",
    "    train_criterion = train_criterion.to(device)\n",
    "\n",
    "\n",
    "    valid_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    collate_fn = None\n",
    "    mixup_transforms = []\n",
    "    ######### Recipe 6 #########\n",
    "    if cfg.mixup:\n",
    "        mixup_transforms.append(T.RandomMixup(num_classes, p=1.0, alpha=config[\"mixup_alpha\"]))\n",
    "    elif cfg.mixup_alpha:\n",
    "        mixup_transforms.append(T.RandomMixup(num_classes, p=1.0, alpha=mixup_alpha))\n",
    "    ######### Recipe 7 #########    \n",
    "    if cfg.cutmix:\n",
    "        mixup_transforms.append(T.RandomCutmix(num_classes, p=1.0, alpha=config[\"cutmix_alpha\"]))\n",
    "    elif cfg.cutmix_alpha:\n",
    "        mixup_transforms.append(T.RandomCutmix(num_classes, p=1.0, alpha=cfg.cutmix_alpha))\n",
    "    if mixup_transforms:\n",
    "        mixupcutmix = torchvision.transforms.RandomChoice(mixup_transforms)\n",
    "        collate_fn = lambda batch: mixupcutmix(*default_collate(batch))\n",
    "        \n",
    "    if checkpoint_dir:\n",
    "        checkpoints = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(checkpoints[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoints[\"optimizer_state\"])\n",
    "        lr_scheduler.load_state_dict(checkpoints[\"lr_scheduler_state\"])\n",
    "        if ema_model:\n",
    "            ema_model.load_state_dict(checkpoints[\"model_ema\"])\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=cfg.batch_size if cfg.batch_size is not None else int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        validset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True)\n",
    "    \n",
    "    \n",
    "    for epoch in range(cfg.epochs): \n",
    "        train_one_epoch(epoch, train_loader, model, ema_model, optimizer, train_criterion, device=device)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        if ema_model:\n",
    "            valid_loss, accuracy = validate(valid_loader, ema_model, valid_criterion, device=device)\n",
    "        else:\n",
    "            valid_loss, accuracy = validate(valid_loader, model, valid_criterion, device=device)\n",
    "                \n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            ckpts = {\"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict(), \"lr_scheduler_state\": lr_scheduler.state_dict()}\n",
    "            if ema_model:\n",
    "                ckpts[\"ema_state\"] = ema_model.state_dict()\n",
    "            torch.save(ckpts, path)\n",
    "            \n",
    "        tune.report(loss=valid_loss, accuracy=accuracy)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31bb74-6227-4889-920d-69db31ceabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, train_loader, model, ema_model, optimizer, criterion, device=\"cpu\"):\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / epoch_steps))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        if ema_model and i % cfg.model_ema_steps == 0:\n",
    "            ema_model.update_parameters(model)\n",
    "            if epoch < cfg.lr_warmup_epochs:\n",
    "                # Reset ema buffer to keep copying weights during warmup period\n",
    "                ema_model.n_averaged.fill_(0)\n",
    "\n",
    "\n",
    "def validate(valid_loader, model, criterion, device=\"cpu\"):\n",
    "    valid_loss = 0.0\n",
    "    valid_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.cpu().numpy()\n",
    "            valid_steps += 1\n",
    "\n",
    "    valid_loss = valid_loss / valid_steps\n",
    "    accuracy = correct / total\n",
    "    return  valid_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f7142-00ac-44c5-96ae-846741ebdf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cfg):\n",
    "    seed = 99\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    data_dir = os.path.abspath(cfg.data_dir)\n",
    "    \n",
    "    config = {}\n",
    "    hyperparam_mutations = {}\n",
    "    \n",
    "    ######### Recipe 1 #########\n",
    "    ### lr optimization ###\n",
    "    if cfg.lr_optim:\n",
    "        config[\"lr\"] = tune.qloguniform(1e-4, 1e-1, 1e-5)\n",
    "        config[\"lr_warmup_decay\"] = tune.qloguniform(1e-5, 1e-3, 1e-6)\n",
    "        config[\"lr_warmup_epochs\"] = tune.qrandint(3, 15, 3)\n",
    "        config[\"batch_size\"] = tune.grid_search([16, 32, 64, 128])\n",
    "        \n",
    "        hyperparam_mutations[\"lr\"] = [1e-4, 1e-1]\n",
    "        hyperparam_mutations[\"lr_warmup_decay\"] = [1e-5, 1e-3]\n",
    "        hyperparam_mutations[\"lr_warmup_epochs\"] = [3, 15]\n",
    "        hyperparam_mutations[\"batch_size\"] = [16, 128]\n",
    "\n",
    "    ######### Recipe 2 #########   \n",
    "    config[\"interpolation\"] = tune.choice([InterpolationMode.BICUBIC, InterpolationMode.BILINEAR, InterpolationMode.NEAREST])\n",
    "    if cfg.trivial:\n",
    "        config[\"num_magnitude_bins\"] = tune.randint(15, 35)\n",
    "        \n",
    "        hyperparam_mutations[\"num_magnitude_bins\"] = [15, 35]\n",
    "    if cfg.random:\n",
    "        config[\"num_ops\"] = tune.randint(2, 5)\n",
    "        config[\"magnitude\"] = tune.randint(5, 9)\n",
    "        config[\"num_magnitude_bins\"] = tune.randint(15, 35)\n",
    "        \n",
    "        hyperparam_mutations[\"num_ops\"] = [2, 5]\n",
    "        hyperparam_mutations[\"magnitude\"] = [5, 9]\n",
    "        hyperparam_mutations[\"num_magnitude_bins\"] = [15, 35]\n",
    "\n",
    "    ######### Recipe 4 #########  \n",
    "    ### data aug ###\n",
    "    if cfg.random_erase:\n",
    "        config[\"random_erase_prob\"] = [0.1, 0.3]\n",
    "        \n",
    "        hyperparam_mutations[\"random_erase_prob\"] = [0.1, 0.3]\n",
    "\n",
    "    ######### Recipe 5 ######### \n",
    "    ### label smoothing\n",
    "    if cfg.label_smoothing:\n",
    "        config[\"label_smoothing\"] = tune.choice([0.05, 0.1, 0.15])\n",
    "        \n",
    "        hyperparam_mutations[\"label_smoothing\"] = [0.05, 0.15]\n",
    "\n",
    "    ######### Recipe 6 #########\n",
    "    ### mixup ###\n",
    "    if cfg.mixup:\n",
    "        config[\"mixup_alpha\"] = tune.quniform(0.1, 0.5, 0.1)\n",
    "        \n",
    "        hyperparam_mutations[\"mixup_alpha\"] = [0.1, 0.5]\n",
    "        \n",
    "    ######### Recipe 7 #########\n",
    "    ### cutmix ###\n",
    "    if cfg.cutmix:\n",
    "        config[\"cutmix_alpha\"] = tune.quniform(0.4, 1.0, 0.1)  \n",
    "        \n",
    "        hyperparam_mutations[\"cutmix_alpha\"] = [0.4, 1.0]\n",
    "\n",
    "    ######### Recipe 8 #########\n",
    "    ### weight decay tuning ###\n",
    "    if cfg.wd_tune:\n",
    "        config[\"weight_decay\"] = tune.qloguniform(1e-5, 1e-3, 1e-6)\n",
    "        \n",
    "        hyperparam_mutations[\"weight_decay\"] = [1e-5, 1e-3]\n",
    "\n",
    "    ######### Recipe 9 #########   \n",
    "    ### fixres ###\n",
    "    if cfg.fixres:\n",
    "        config[\"train_crop_size\"] = tune.grid_search([176, 192, 208])   \n",
    "        \n",
    "        hyperparam_mutations[\"train_crop_size\"] =[176, 208]\n",
    "\n",
    "    ######### Recipe 10 #########\n",
    "    ### model ema ###\n",
    "    if cfg.ema:\n",
    "        config[\"model_ema_steps\"] = tune.qrandint(15, 50, 5)\n",
    "        config[\"model_ema_decay\"] = tune.uniform(0.99, 0.99998)\n",
    "        \n",
    "        hyperparam_mutations[\"model_ema_steps\"] = [15, 50]\n",
    "        hyperparam_mutations[\"model_ema_decay\"] = [0.99, 0.99998]\n",
    "        \n",
    "    print(config)\n",
    "    load_data(config, data_dir)\n",
    "    \n",
    "    if cfg.asha:\n",
    "        scheduler = ASHAScheduler(\n",
    "            time_attr=\"training_iteration\",\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            max_t=cfg.epochs,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2\n",
    "        )\n",
    "    \n",
    "    if cfg.pbt:\n",
    "        scheduler = scheduler = PopulationBasedTraining(\n",
    "            time_attr=\"training_iteration\",\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            perturbation_interval=300.0,\n",
    "            hyperparam_mutations=hyperparam_mutations\n",
    "        )\n",
    "        \n",
    "    if cfg.pb2:\n",
    "        scheduler = pb2.PB2(\n",
    "            time_attr=\"training_iteration\",\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            perturbation_interval=300.0,\n",
    "            hyperparam_bounds=hyperparam_mutations\n",
    "        )\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "        \n",
    "    result = tune.run(\n",
    "        partial(train, data_dir=data_dir),\n",
    "        name=cfg.name,\n",
    "        resources_per_trial={\"cpu\": cfg.cpus_per_trial, \"gpu\": cfg.gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=cfg.num_samples,\n",
    "        scheduler=scheduler,\n",
    "        stop={\"accuracy\": 0.99},\n",
    "        resume=\"AUTO\",\n",
    "        progress_reporter=reporter\n",
    "    )\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade02543-ffc0-444c-967c-8354d31744ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = simpleargs\n",
    "cfg.data_dir=\"./hymenoptera_data/\"\n",
    "# cfg.seed=99\n",
    "cfg.model=\"tiny\"\n",
    "cfg.num_samples=3\n",
    "cfg.epochs=3\n",
    "cfg.cpus_per_trial=2\n",
    "cfg.gpus_per_trial=0\n",
    "cfg.name=\"hparams_tune\"\n",
    "###### scheduler type\n",
    "cfg.asha=False\n",
    "cfg.pbt=False\n",
    "cfg.pb2=True\n",
    "###### pass to instantiate recipe                   \n",
    "cfg.lr_optim=True\n",
    "cfg.trivial=False\n",
    "cfg.random=False\n",
    "cfg.random_erase=False\n",
    "cfg.label_smoothing=False\n",
    "cfg.mixup=False\n",
    "cfg.cutmix=False\n",
    "cfg.wd_tune=False\n",
    "cfg.fixres=False\n",
    "cfg.ema=False\n",
    "cfg.bce=True\n",
    "###### pass to use optimized hparam  \n",
    "cfg.batch_size=None\n",
    "cfg.lr=None\n",
    "cfg.lr_warmup_epochs=None\n",
    "cfg.lr_warmup_decay=None\n",
    "cfg.weight_decay=None\n",
    "cfg.smooth=None\n",
    "cfg.mixup_alpha=None\n",
    "cfg.cutmix_alpha=None\n",
    "cfg.random_erase_prob=None\n",
    "cfg.model_ema_steps=None\n",
    "cfg.model_ema_decay=None\n",
    "cfg.train_crop_size=None\n",
    "cfg.interpolation=None\n",
    "cfg.num_ops=None\n",
    "cfg.magnitude=None\n",
    "cfg.num_magnitude_bins=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d2dfa-0ac5-47c0-946e-26310ac7cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### seed for ray[tune] schedulers\n",
    "seed = 99\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# You can change the number of GPUs per trial here:\n",
    "result = main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bccaab-fbd7-4b15-b1bb-3a8af563d733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a28e11-9ca0-4b4c-b891-a46f66514c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3453b8-87b8-4fb2-b2f7-789704ff54b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
