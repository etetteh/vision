{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58205394-d84b-44a9-82a8-63d5cb752607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, torchvision.transforms\n",
    "import sklearn, sklearn.model_selection\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import timm\n",
    "import torch, torchvision\n",
    "\n",
    "from torch import nn\n",
    "from timm.loss import BinaryCrossEntropy, LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import autoaugment, transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "import transforms as T\n",
    "import utils\n",
    "\n",
    "import torchxrayvision as xrv\n",
    "from torchxrayvision.datasets import Dataset\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining, pb2, AsyncHyperBandScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38c51fb-38c7-4405-bd1d-00921c84d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    <torchxrayvision.datasets.ToPILImage object at 0x7fdd80805700>\n",
      "    RandomAffine(degrees=[-45.0, 45.0], translate=(0.15, 0.15), scale=(0.85, 1.15))\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "seed=99\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "    \n",
    "data_aug_rot = 45\n",
    "data_aug_trans = 0.15\n",
    "data_aug_scale = 0.15\n",
    "\n",
    "data_aug = torchvision.transforms.Compose([\n",
    "        xrv.datasets.ToPILImage(),\n",
    "        # transforms.Grayscale(num_output_channels=3),\n",
    "        torchvision.transforms.RandomAffine(data_aug_rot, \n",
    "                                            translate=(data_aug_trans, data_aug_trans), \n",
    "                                            scale=(1.0-data_aug_scale, 1.0+data_aug_scale)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "print(data_aug)\n",
    "\n",
    "test_augment = transforms.Compose([\n",
    "                xrv.datasets.ToPILImage(),\n",
    "                # transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.PILToTensor(),\n",
    "                ])\n",
    "\n",
    "data_transform = torchvision.transforms.Compose([xrv.datasets.XRayCenterCrop(),xrv.datasets.XRayResizer(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165de904-0685-4b65-ad8a-ed1277b674a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../../../Music/work/data/\"\n",
    "\n",
    "# if \"openi\" in cfg.dataset:\n",
    "OPENI_dataset = xrv.datasets.Openi_Dataset(\n",
    "        imgpath=dataset_dir + \"/OpenI/images/\",\n",
    "        xmlpath=dataset_dir + \"NLMCXR_reports.tgz\", \n",
    "        dicomcsv_path=dataset_dir + \"nlmcxr_dicom_metadata.csv.gz\",\n",
    "        tsnepacsv_path=dataset_dir + \"nlmcxr_tsne_pa.csv.gz\",\n",
    "        transform=None, data_aug=None)\n",
    "# if \"rsna\" in cfg.dataset:\n",
    "RSNA_dataset = xrv.datasets.RSNA_Pneumonia_Dataset(\n",
    "        imgpath=dataset_dir + \"/kaggle-pneumonia-jpg/stage_2_train_images_jpg\",\n",
    "        csvpath=dataset_dir + \"/kaggle-pneumonia-jpg/stage_2_train_labels.csv\",\n",
    "        dicomcsvpath=dataset_dir + \"kaggle_stage_2_train_images_dicom_headers.csv.gz\",\n",
    "        transform=None, data_aug=None, unique_patients=False)\n",
    "        \n",
    "# if \"google\" in cfg.dataset:\n",
    "Google_dataset = xrv.datasets.NIH_Google_Dataset(\n",
    "    imgpath=dataset_dir + \"/images-224-NIH\",\n",
    "    csvpath=dataset_dir + \"/google2019_nih-chest-xray-labels.csv.gz\", \n",
    "    transform=None, data_aug=None)\n",
    "    \n",
    "NIH_dataset = xrv.datasets.NIH_Dataset(\n",
    "        imgpath=dataset_dir + \"/images-224-NIH\", \n",
    "        csvpath=dataset_dir + \"/Data_Entry_2017_v2020.csv.gz\",\n",
    "        bbox_list_path=dataset_dir + \"/BBox_List_2017.csv.gz\",\n",
    "        transform=None, data_aug=None, unique_patients=False)\n",
    "\n",
    "## Load CHEXPERT Dataset ###\n",
    "CHEX_dataset = xrv.datasets.CheX_Dataset(\n",
    "        imgpath=dataset_dir + \"/CheXpert-v1.0-small\",\n",
    "        csvpath=dataset_dir + \"/CheXpert-v1.0-small/train.csv\",\n",
    "        transform=None, data_aug=None, unique_patients=False)\n",
    "\n",
    "# ### Load MIMIC_CH Dataset ###\n",
    "MIMIC_CH_dataset = xrv.datasets.MIMIC_Dataset(\n",
    "    imgpath=dataset_dir + \"/images-224-MIMIC/files\",\n",
    "    csvpath=dataset_dir + \"/MIMICCXR-2.0/mimic-cxr-2.0.0-chexpert.csv.gz\",\n",
    "    metacsvpath=dataset_dir + \"/MIMICCXR-2.0/mimic-cxr-2.0.0-metadata.csv.gz\",\n",
    "    transform=None, data_aug=None, unique_patients=False)\n",
    "\n",
    "### Load PADCHEST Dataset ###\n",
    "PC_dataset = xrv.datasets.PC_Dataset(\n",
    "        imgpath=dataset_dir + \"/PC/images-224\",\n",
    "        csvpath=dataset_dir + \"/PC/PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv\",\n",
    "        transform=None, data_aug=None, unique_patients=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83f8df7-80ca-49c0-9f2a-071f44456e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(dataset, seed):\n",
    "    if \"patientid\" not in dataset.csv:\n",
    "        dataset.csv[\"patientid\"] = [\"{}-{}\".format(dataset.__class__.__name__, i) for i in range(len(dataset))]\n",
    "        \n",
    "    gss = sklearn.model_selection.GroupShuffleSplit(train_size=0.8,test_size=0.2, random_state=seed)\n",
    "    \n",
    "    traininds, test_inds = next(gss.split(X=range(len(dataset)), groups=dataset.csv.patientid))\n",
    "    traindataset = xrv.datasets.SubsetDataset(dataset, traininds)\n",
    "    test_dataset = xrv.datasets.SubsetDataset(dataset, test_inds)\n",
    "    \n",
    "    train_inds, valid_inds = next(gss.split(X=range(len(traindataset)), groups=traindataset.csv.patientid))\n",
    "    train_dataset = xrv.datasets.SubsetDataset(traindataset, train_inds)\n",
    "    valid_dataset = xrv.datasets.SubsetDataset(traindataset, valid_inds)\n",
    "\n",
    "    num_classes = len(dataset.pathologies)\n",
    "    return train_dataset, valid_dataset, test_dataset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e47c36-7bbd-4ad5-9b5e-4821ad48c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset, num_classes = create_splits(Google_dataset, seed=seed)\n",
    "\n",
    "train_dataset.transform, train_dataset.data_aug = data_transform, data_aug\n",
    "valid_dataset.transform, valid_dataset.data_aug = data_transform, test_augment\n",
    "test_dataset.transform, test_dataset.data_aug = data_transform, test_augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3986931e-cced-403e-b054-3ed84378ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(train_dataset) + len(valid_dataset) + len(test_dataset) == len(NIH_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc388ac-642d-44ab-b1e6-e3acd694910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(num_classes=num_classes, pretrained=False)\n",
    "#patch for single channel\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "model = torch.hub.load(\"facebookresearch/deit:main\", \"deit_small_patch16_224\", pretrained=False)\n",
    "model.patch_embed.proj = torch.nn.Conv2d(1, model.patch_embed.proj.out_channels, kernel_size=(16, 16), stride=(16, 16))\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# num_ftrs = model.head.in_features\n",
    "# model.head = nn.Linear(num_ftrs, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583f2ad-edab-4979-9858-432ce03f2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scripted_model = torch.jit.script(model)\n",
    "backend = \"fbgemm\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n",
    "scripted_quantized_model = torch.jit.script(quantized_model)\n",
    "\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7fb0dd-a502-42c1-b8cd-3c04c952889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=8,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4, \n",
    "                                           pin_memory=True)\n",
    "\n",
    "# Optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5, amsgrad=True)\n",
    "print(optim)\n",
    "\n",
    "criterion = BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba71631a-c689-458d-a425-ee8fca7ac43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm_base\n",
    "def tqdm(*args, **kwargs):\n",
    "    if hasattr(tqdm_base, '_instances'):\n",
    "        for instance in list(tqdm_base._instances):\n",
    "            tqdm_base._decr_instances(instance)\n",
    "    return tqdm_base(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a3cff1-5c9e-484a-baca-ced668fcafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, device, train_loader, optimizer, criterion, limit=None):\n",
    "    model.train()\n",
    "\n",
    "    weights = np.nansum(train_loader.dataset.labels, axis=0)\n",
    "    weights = weights.max() - weights + weights.mean()\n",
    "    weights = weights/weights.max()\n",
    "    weights = torch.from_numpy(weights).to(device).float()\n",
    "    # print(\"task weights\", weights)\n",
    "    \n",
    "    avg_loss = []\n",
    "    t = tqdm(train_loader)\n",
    "    for batch_idx, samples in enumerate(t):\n",
    "        \n",
    "        if limit and (batch_idx > limit):\n",
    "            print(\"breaking out\")\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = samples[\"img\"].float().to(device)\n",
    "        targets = samples[\"lab\"].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = torch.zeros(1).to(device).float()\n",
    "        for task in range(targets.shape[1]):\n",
    "            task_output = outputs[:,task]\n",
    "            task_target = targets[:,task]\n",
    "            mask = ~torch.isnan(task_target)\n",
    "            task_output = task_output[mask]\n",
    "            task_target = task_target[mask]\n",
    "            if len(task_target) > 0:\n",
    "                task_loss = criterion(task_output.float(), task_target.float())\n",
    "                loss += weights[task]*task_loss\n",
    "\n",
    "        \n",
    "        loss = loss.sum()       \n",
    "        loss.backward()\n",
    "\n",
    "        avg_loss.append(loss.detach().cpu().numpy())\n",
    "        t.set_description(f'Epoch {epoch + 1} - Train - Loss = {np.mean(avg_loss):4.4f}')\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6684d0-2b97-436c-ba80-08add841bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train - Loss = 2.9029:  63%|█████████▍     | 41/65 [01:21<00:47,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train - Loss = 1.1174:  23%|███▍           | 15/65 [00:32<01:48,  2.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15439/1902064296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     avg_loss = train_epoch(\n\u001b[0m\u001b[1;32m      4\u001b[0m                            \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15439/691642915.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, device, train_loader, optimizer, criterion, limit)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1} - Train - Loss = {np.mean(avg_loss):4.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "for epoch in range(0, 10):\n",
    "    avg_loss = train_epoch(\n",
    "                           epoch=epoch,\n",
    "                           model=model,\n",
    "                           device=device,\n",
    "                           optimizer=optim,\n",
    "                           train_loader=train_loader,\n",
    "                           criterion=criterion,\n",
    "                           limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677d4ef-84cc-4c1c-b790-25c026f87197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a400ecc-812f-4be8-861d-6409086e3747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f630ec-a2cf-49b5-b07b-ab835a887bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd78df8-e989-488d-9655-88da63f80581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903e6f1-ea0d-4004-b019-1d01c1ab5e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ea2da-e416-4e4a-ae12-61cab0161b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884e1c6-2da1-46c8-9a59-a2cb1cf754ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58578177-17c2-4b0f-a7b8-fd6102c4ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(dataset, seed):\n",
    "    if \"patientid\" not in dataset.csv:\n",
    "        dataset.csv[\"patientid\"] = [\"{}-{}\".format(dataset.__class__.__name__, i) for i in range(len(dataset))]\n",
    "        \n",
    "    gss = sklearn.model_selection.GroupShuffleSplit(train_size=0.8,test_size=0.2, random_state=seed)\n",
    "    \n",
    "    traininds, test_inds = next(gss.split(X=range(len(dataset)), groups=dataset.csv.patientid))\n",
    "    traindataset = xrv.datasets.SubsetDataset(dataset, traininds)\n",
    "    test_dataset = xrv.datasets.SubsetDataset(dataset, test_inds)\n",
    "    \n",
    "    train_inds, valid_inds = next(gss.split(X=range(len(traindataset)), groups=traindataset.csv.patientid))\n",
    "    train_dataset = xrv.datasets.SubsetDataset(traindataset, train_inds)\n",
    "    valid_dataset = xrv.datasets.SubsetDataset(traindataset, valid_inds)\n",
    "\n",
    "    num_classes = len(dataset.pathologies)\n",
    "    return train_dataset, valid_dataset, test_dataset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81560514-0b9c-451a-850d-64f1c3d07b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(config, data_dir='./data'):\n",
    "    \n",
    "    ### Load NIH Dataset ###\n",
    "    if \"nih\" in cfg.dataset:\n",
    "        dataset = xrv.datasets.NIH_Dataset(\n",
    "            imgpath=cfg.dataset_dir + \"/images-224-NIH\", \n",
    "            csvpath=cfg.dataset_dir + \"/Data_Entry_2017_v2020.csv.gz\",\n",
    "            bbox_list_path=cfg.dataset_dir + \"/BBox_List_2017.csv.gz\",\n",
    "            transform=None, \n",
    "            data_aug=None,\n",
    "            unique_patients=False\n",
    "        )\n",
    "\n",
    "    ### Load CHEXPERT Dataset ###\n",
    "    if \"chex\" in cfg.dataset:\n",
    "        dataset = xrv.datasets.CheX_Dataset(\n",
    "            imgpath=cfg.dataset_dir + \"/CheXpert-v1.0-small\",\n",
    "            csvpath=cfg.dataset_dir + \"/CheXpert-v1.0-small/train.csv\",\n",
    "            transform=None,\n",
    "            data_aug=None,\n",
    "            unique_patients=False\n",
    "        )\n",
    "\n",
    "    ### Load MIMIC_CH Dataset ###\n",
    "    if \"mimic\" in cfg.dataset:\n",
    "        dataset = xrv.datasets.MIMIC_Dataset(\n",
    "            imgpath=cfg.dataset_dir + \"/images-224-MIMIC/files\",\n",
    "            csvpath=cfg.dataset_dir + \"/MIMICCXR-2.0/mimic-cxr-2.0.0-chexpert.csv.gz\",\n",
    "            metacsvpath=cfg.dataset_dir + \"/MIMICCXR-2.0/mimic-cxr-2.0.0-metadata.csv.gz\",\n",
    "            transform=None,\n",
    "            data_aug=None,\n",
    "            unique_patients=False\n",
    "        )\n",
    "\n",
    "    ### Load PADCHEST Dataset ###\n",
    "    if \"pc\" in cfg.dataset:\n",
    "        dataset = xrv.datasets.PC_Dataset(\n",
    "            imgpath=cfg.dataset_dir + \"/PC/images-224\",\n",
    "            csvpath=cfg.dataset_dir + \"/PC/PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv\",\n",
    "            transform=None,\n",
    "            data_aug=None,\n",
    "            unique_patients=False\n",
    "        )\n",
    "\n",
    "    if \"google\" in cfg.dataset:\n",
    "        dataset = xrv.datasets.NIH_Google_Dataset(\n",
    "            imgpath=cfg.dataset_dir + \"/images-224-NIH\",\n",
    "            csvpath=cfg.dataset_dir + \"/google2019_nih-chest-xray-labels.csv.gz\", \n",
    "            transform=None, \n",
    "            data_aug=None\n",
    "        )\n",
    "        \n",
    "    if \"rsna\" in cfg.dataset:\n",
    "        dataset = xrv.datasets.RSNA_Pneumonia_Dataset(\n",
    "                imgpath=cfg.dataset_dir + \"/kaggle-pneumonia-jpg/stage_2_train_images_jpg\",\n",
    "                csvpath=cfg.dataset_dir + \"/kaggle-pneumonia-jpg/stage_2_train_labels.csv\",\n",
    "                dicomcsvpath=cfg.dataset_dir + \"kaggle_stage_2_train_images_dicom_headers.csv.gz\",\n",
    "                transform=None,\n",
    "                data_aug=None,\n",
    "                unique_patients=False\n",
    "        )\n",
    "        \n",
    "    if \"openi\" in cfg.dataset:\n",
    "        OPENI_dataset = xrv.datasets.Openi_Dataset(\n",
    "                imgpath=cfg.dataset_dir + \"/OpenI/images/\",\n",
    "                xmlpath=cfg.dataset_dir + \"NLMCXR_reports.tgz\", \n",
    "                dicomcsv_path=cfg.dataset_dir + \"nlmcxr_dicom_metadata.csv.gz\",\n",
    "                tsnepacsv_path=cfg.dataset_dir + \"nlmcxr_tsne_pa.csv.gz\",\n",
    "                transform=None,\n",
    "                data_aug=None\n",
    "        )\n",
    "    #############################################################################    \n",
    "    augment = [\n",
    "               xrv.datasets.ToPILImage(),\n",
    "               transforms.Grayscale(num_output_channels=3),\n",
    "               transforms.RandomHorizontalFlip()\n",
    "               ]\n",
    "\n",
    "    ######### Recipe 2 #########           \n",
    "    if cfg.random:\n",
    "        augment.append(autoaugment.RandAugment(num_ops=config[\"num_ops\"], magnitude=config[\"magnitude\"], num_magnitude_bins=config[\"num_magnitude_bins\"], interpolation=config[\"interpolation\"]))\n",
    "    elif cfg.num_ops:\n",
    "        augment.append(autoaugment.RandAugment(num_ops=cfg.num_ops, magnitude=cfg.magnitude, num_magnitude_bins=cfg.num_magnitude_bins, interpolation=cfg.interpolation))\n",
    "\n",
    "    if cfg.trivial:\n",
    "        augment.append(autoaugment.TrivialAugmentWide(num_magnitude_bins=config[\"num_magnitude_bins\"], interpolation=config[\"interpolation\"]))\n",
    "    elif cfg.num_magnitude_bins:\n",
    "        augment.append(autoaugment.TrivialAugmentWide(num_magnitude_bins=cfg.num_magnitude_bins, interpolation=cfg.interpolation))\n",
    "\n",
    "    augment.extend([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            ])  \n",
    "    ######### Recipe 4 #########\n",
    "    if cfg.random_erase:\n",
    "        augment.append(transforms.RandomErasing(p=config[\"random_erase_prob\"]))\n",
    "    elif cfg.random_erase_prob:\n",
    "        augment.append(transforms.RandomErasing(p=cfg.random_erase_prob))\n",
    "\n",
    "    augment = transforms.Compose(augment)\n",
    "\n",
    "    test_augment = transforms.Compose([\n",
    "                xrv.datasets.ToPILImage(),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.PILToTensor(),\n",
    "                transforms.ConvertImageDtype(torch.float),\n",
    "                ])\n",
    "\n",
    "    data_transform = torchvision.transforms.Compose([xrv.datasets.XRayCenterCrop(), xrv.datasets.XRayResizer(224)])\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset, num_classes = create_splits(dataset, seed=cfg.seed)\n",
    "\n",
    "    train_dataset.transform, train_dataset.data_aug = data_transorms, augment\n",
    "    valid_dataset.transform, valid_dataset.data_aug = data_transorms, test_augment\n",
    "    test_dataset.transform, test_dataset.data_aug = data_transorms, test_augment\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccdf366-2888-4542-bce9-f65bc6f790ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c1302-ffb4-4dec-bf41-01ea593ee4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(config, epoch, model, ema_model, device, train_loader, optimizer, criterion, limit=None):\n",
    "    model.train()\n",
    "\n",
    "    weights = np.nansum(train_loader.dataset.labels, axis=0)\n",
    "    weights = weights.max() - weights + weights.mean()\n",
    "    weights = weights/weights.max()\n",
    "    weights = torch.from_numpy(weights).to(device).float()\n",
    "    print(\"task weights\", weights)\n",
    "    \n",
    "    avg_loss = []\n",
    "    t = tqdm(train_loader)\n",
    "    for batch_idx, samples in enumerate(t):\n",
    "        \n",
    "        if limit and (batch_idx > limit):\n",
    "            print(\"breaking out\")\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = samples[\"img\"].float().to(device)\n",
    "        targets = samples[\"lab\"].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = torch.zeros(1).to(device).float()\n",
    "        for task in range(targets.shape[1]):\n",
    "            task_output = outputs[:,task]\n",
    "            task_target = targets[:,task]\n",
    "            mask = ~torch.isnan(task_target)\n",
    "            task_output = task_output[mask]\n",
    "            task_target = task_target[mask]\n",
    "            if len(task_target) > 0:\n",
    "                task_loss = criterion(task_output.float(), task_target.float())\n",
    "                loss += weights[task]*task_loss\n",
    "                \n",
    "        loss = loss.sum()        \n",
    "        loss.backward()\n",
    "\n",
    "        avg_loss.append(loss.detach().cpu().numpy())\n",
    "        t.set_description(f'Epoch {epoch + 1} - Train - Loss = {np.mean(avg_loss):4.4f}')\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if cfg.model_ema_steps:\n",
    "            if ema_model and i % cfg.model_ema_steps == 0:\n",
    "                ema_model.update_parameters(model)\n",
    "                if epoch < cfg.lr_warmup_epochs:\n",
    "                    ema_model.n_averaged.fill_(0)\n",
    "        else:           \n",
    "            if ema_model and batch_idx % config[\"model_ema_steps\"] == 0:\n",
    "                ema_model.update_parameters(model)\n",
    "                if epoch < config[\"lr_warmup_epochs\"]:\n",
    "                    ema_model.n_averaged.fill_(0)\n",
    "\n",
    "    return np.mean(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915d4ab-10f9-4655-b8db-bc57536c2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_test_epoch(name, epoch, model, device, data_loader, criterion, limit=None):\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss = []\n",
    "    task_outputs={}\n",
    "    task_targets={}\n",
    "    for task in range(data_loader.dataset[0][\"lab\"].shape[0]):\n",
    "        task_outputs[task] = []\n",
    "        task_targets[task] = []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        t = tqdm(data_loader)\n",
    "        for batch_idx, samples in enumerate(t):\n",
    "\n",
    "            if limit and (batch_idx >= limit):\n",
    "                print(\"breaking out\")\n",
    "                break\n",
    "            \n",
    "            images = samples[\"img\"].to(device)#.cuda()\n",
    "            targets = samples[\"lab\"].to(device)#.cuda()\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = torch.zeros(1).to(device).double()\n",
    "            for task in range(targets.shape[1]):\n",
    "                task_output = outputs[:,task]\n",
    "                task_target = targets[:,task]\n",
    "                mask = ~torch.isnan(task_target)\n",
    "                task_output = task_output[mask]\n",
    "                task_target = task_target[mask]\n",
    "                if len(task_target) > 0:\n",
    "                    loss += criterion(task_output.double(), task_target.double())\n",
    "                \n",
    "                task_outputs[task].append(task_output.detach().cpu().numpy())\n",
    "                task_targets[task].append(task_target.detach().cpu().numpy())\n",
    "\n",
    "            loss = loss.sum()\n",
    "            \n",
    "            avg_loss.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            if epoch is not None:\n",
    "                t.set_description(f'Epoch {epoch + 1} - {name} - Loss = {np.mean(avg_loss):4.4f}')              \n",
    "            \n",
    "        for task in range(len(task_targets)):\n",
    "            task_outputs[task] = np.concatenate(task_outputs[task])\n",
    "            task_targets[task] = np.concatenate(task_targets[task])\n",
    "    \n",
    "        task_aucs = []\n",
    "        for task in range(len(task_targets)):\n",
    "            if len(np.unique(task_targets[task]))> 1:\n",
    "                task_auc = sklearn.metrics.roc_auc_score(task_targets[task], task_outputs[task])\n",
    "                task_aucs.append(task_auc)\n",
    "            else:\n",
    "                task_aucs.append(np.nan)\n",
    "\n",
    "    task_aucs = np.asarray(task_aucs)\n",
    "    auc = np.mean(task_aucs[~np.isnan(task_aucs)])\n",
    "    \n",
    "    if epoch is not None:\n",
    "        print(f'Epoch {epoch + 1} - {name} - Avg AUC = {auc:4.4f}')\n",
    "    else:\n",
    "        print(f'{name} - Avg AUC = {auc:4.4f}')\n",
    "\n",
    "    return auc, np.mean(avg_loss), task_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36163c9-5ceb-4d6a-aa8d-c03c09e3f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, checkpoint_dir=None, data_dir=None):\n",
    "    random.seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.random.manual_seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(cfg.seed)\n",
    "    \n",
    "    train_dataset, valid_dataset, _, num_classes = load_data(config, data_dir=data_dir)\n",
    "    \n",
    "    model = torch.hub.load(\"facebookresearch/deit:main\", \"deit_\"+cfg.model+\"_patch16_224\", pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    num_ftrs = model.head.in_features\n",
    "    model.head = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    ######### Recipe 10 #########\n",
    "    ema_model = None\n",
    "    if cfg.ema:\n",
    "        adjust = 1 * cfg.batch_size * config[\"model_ema_steps\"] / cfg.epochs\n",
    "        alpha = 1.0 - config[\"model_ema_decay\"]\n",
    "        alpha = min(1.0, alpha * adjust)\n",
    "        ema_model = utils.ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "    elif (cfg.model_ema_decay and cfg.model_ema_steps):\n",
    "        adjust = 1 * cfg.batch_size * cfg.model_ema_steps / cfg.epochs\n",
    "        alpha = 1.0 - cfg.model_ema_decay\n",
    "        alpha = min(1.0, alpha * adjust)\n",
    "        ema_model = utils.ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "        \n",
    "    \n",
    "    parameters = model.head.parameters()\n",
    "    \n",
    "    ######### Recipe 8 #########                   \n",
    "    if cfg.wd_tune:\n",
    "        norm_weight_decay=0.0\n",
    "        param_groups = utils.split_normalization_params(model)\n",
    "        wd_groups = [norm_weight_decay, config[\"weight_decay\"]]\n",
    "        parameters = [{\"params\": p, \"weight_decay\": w} for p, w in zip(param_groups, wd_groups) if p]\n",
    "    elif cfg.weight_decay:\n",
    "        norm_weight_decay=0.0\n",
    "        param_groups = utils.split_normalization_params(model)\n",
    "        wd_groups = [norm_weight_decay, cfg.weight_decay]\n",
    "        parameters = [{\"params\": p, \"weight_decay\": w} for p, w in zip(param_groups, wd_groups) if p]\n",
    "    \n",
    "    \n",
    "    ######### Recipe 1 #########\n",
    "    if cfg.lr_optim:\n",
    "        optimizer = torch.optim.AdamW(parameters, lr=config[\"lr\"])\n",
    "        main_lr_scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs - config[\"lr_warmup_epochs\"])\n",
    "        warmup_lr_scheduler = LinearLR(optimizer, start_factor=config[\"lr_warmup_decay\"], total_iters=config[\"lr_warmup_epochs\"])\n",
    "        lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[config[\"lr_warmup_epochs\"]])\n",
    "    elif cfg.lr:\n",
    "        optimizer = torch.optim.AdamW(parameters, lr=cfg.lr)\n",
    "        main_lr_scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs - cfg.lr_warmup_epochs)\n",
    "        warmup_lr_scheduler = LinearLR(optimizer, start_factor=cfg.lr_warmup_decay, total_iters=cfg.lr_warmup_epochs)\n",
    "        lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[cfg.lr_warmup_epochs])\n",
    "        \n",
    "    ######### Recipe 5 #########\n",
    "    if cfg.mixup or cfg.cutmix:\n",
    "        train_criterion = BinaryCrossEntropy(smoothing=0.0)\n",
    "    elif cfg.label_smoothing:\n",
    "        if cfg.smooth:\n",
    "            train_criterion = BinaryCrossEntropy(smoothing=cfg.smooth)\n",
    "        else:\n",
    "            train_criterion = BinaryCrossEntropy(smoothing=config[\"label_smoothing\"])\n",
    "    else:\n",
    "        train_criterion = BinaryCrossEntropy(smoothing=0.0)\n",
    "    train_criterion = train_criterion.to(device)\n",
    "\n",
    "    valid_criterion = BinaryCrossEntropy(smoothing=0.0).to(device)\n",
    "    \n",
    "    collate_fn = None\n",
    "    mixup_transforms = []\n",
    "    ######### Recipe 6 #########\n",
    "    if cfg.mixup:\n",
    "        mixup_transforms.append(T.RandomMixup(num_classes, p=1.0, alpha=config[\"mixup_alpha\"]))\n",
    "    elif cfg.mixup_alpha:\n",
    "        mixup_transforms.append(T.RandomMixup(num_classes, p=1.0, alpha=mixup_alpha))\n",
    "    ######### Recipe 7 #########    \n",
    "    if cfg.cutmix:\n",
    "        mixup_transforms.append(T.RandomCutmix(num_classes, p=1.0, alpha=config[\"cutmix_alpha\"]))\n",
    "    elif cfg.cutmix_alpha:\n",
    "        mixup_transforms.append(T.RandomCutmix(num_classes, p=1.0, alpha=cfg.cutmix_alpha))\n",
    "    if mixup_transforms:\n",
    "        mixupcutmix = torchvision.transforms.RandomChoice(mixup_transforms)\n",
    "        collate_fn = lambda batch: mixupcutmix(*default_collate(batch))\n",
    "        \n",
    "    if checkpoint_dir:\n",
    "        checkpoints = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(checkpoints[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoints[\"optimizer_state\"])\n",
    "        lr_scheduler.load_state_dict(checkpoints[\"lr_scheduler_state\"])\n",
    "        if ema_model:\n",
    "            ema_model.load_state_dict(checkpoints[\"model_ema\"])\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size if cfg.batch_size is not None else int(config[\"batch_size\"]),\n",
    "        sampler=torch.utils.data.RandomSampler(train_dataset),\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=cfg.batch_size if cfg.batch_size is not None else int(config[\"batch_size\"]),\n",
    "        sampler=torch.utils.data.SequentialSampler(valid_dataset),\n",
    "        num_workers=2,\n",
    "        pin_memory=True)\n",
    "    \n",
    "    \n",
    "    for epoch in range(cfg.epochs): \n",
    "        train_epoch(config, epoch, model, ema_model, device, train_loader, optimizer, criterion, limit=cfg.limit)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        if ema_model:\n",
    "            valid_auc, valid_loss, _ = valid_test_epoch(name, epoch, ema_model, device, data_loader, valid_criterion, limit=cfg.limit)\n",
    "        else:\n",
    "            valid_auc, valid_loss, _ = valid_test_epoch(name, epoch, model, device, data_loader, valid_criterion, limit=cfg.limit)\n",
    "                \n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            ckpts = {\"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict(), \"lr_scheduler_state\": lr_scheduler.state_dict()}\n",
    "            if ema_model:\n",
    "                ckpts[\"ema_state\"] = ema_model.state_dict()\n",
    "            torch.save(ckpts, path)\n",
    "            \n",
    "        tune.report(Loss=valid_loss, AUC=valid_auc)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f858054-e69b-4eb4-b2ab-5ed91b118e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cfg):\n",
    "    random.seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.random.manual_seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    data_dir = os.path.abspath(cfg.data_dir)\n",
    "    \n",
    "    config = {}\n",
    "    hyperparam_mutations = {}\n",
    "    \n",
    "    ######### Recipe 1 #########\n",
    "    ### lr optimization ###\n",
    "    if cfg.lr_optim:\n",
    "        config[\"lr\"] = tune.qloguniform(1e-4, 1e-1, 1e-5)\n",
    "        config[\"lr_warmup_decay\"] = tune.qloguniform(1e-5, 1e-2, 1e-6)\n",
    "        config[\"lr_warmup_epochs\"] = tune.randint(3, 15)\n",
    "        config[\"batch_size\"] = tune.choice([16, 32, 64])\n",
    "        \n",
    "        hyperparam_mutations[\"lr\"] = [1e-4, 1e-1]\n",
    "        hyperparam_mutations[\"lr_warmup_decay\"] = [1e-5, 1e-2]\n",
    "        hyperparam_mutations[\"lr_warmup_epochs\"] = [3, 15]\n",
    "        hyperparam_mutations[\"batch_size\"] = [16, 64]\n",
    "\n",
    "    ######### Recipe 2 #########   \n",
    "    if cfg.trivial:\n",
    "        config[\"interpolation\"] = tune.choice([InterpolationMode.BICUBIC, InterpolationMode.BILINEAR, InterpolationMode.NEAREST])\n",
    "        config[\"num_magnitude_bins\"] = tune.randint(5, 30)\n",
    "        \n",
    "        hyperparam_mutations[\"num_magnitude_bins\"] = [5, 30]\n",
    "    if cfg.random:\n",
    "        config[\"interpolation\"] = tune.choice([InterpolationMode.BICUBIC, InterpolationMode.BILINEAR, InterpolationMode.NEAREST])\n",
    "        config[\"num_ops\"] = tune.randint(2, 5)\n",
    "        config[\"magnitude\"] = tune.randint(5, 9)\n",
    "        config[\"num_magnitude_bins\"] = tune.randint(5, 30)\n",
    "        \n",
    "        hyperparam_mutations[\"num_ops\"] = [2, 5]\n",
    "        hyperparam_mutations[\"magnitude\"] = [5, 9]\n",
    "        hyperparam_mutations[\"num_magnitude_bins\"] = [5, 30]\n",
    "\n",
    "    ######### Recipe 4 #########  \n",
    "    ### data aug ###\n",
    "    if cfg.random_erase:\n",
    "        config[\"random_erase_prob\"] = [0.1, 0.2, 0.3]\n",
    "        \n",
    "        hyperparam_mutations[\"random_erase_prob\"] = [0.1, 0.2, 0.3]\n",
    "\n",
    "    ######### Recipe 5 ######### \n",
    "    ### label smoothing\n",
    "    if cfg.label_smoothing:\n",
    "        config[\"label_smoothing\"] = tune.choice([0.05, 0.1, 0.15])\n",
    "        \n",
    "        hyperparam_mutations[\"label_smoothing\"] = [0.05, 0.15]\n",
    "\n",
    "    ######### Recipe 6 #########\n",
    "    ### mixup ###\n",
    "    if cfg.mixup:\n",
    "        config[\"mixup_alpha\"] = tune.quniform(0.1, 0.5, 0.1)\n",
    "        \n",
    "        hyperparam_mutations[\"mixup_alpha\"] = [0.1, 0.5]\n",
    "        \n",
    "    ######### Recipe 7 #########\n",
    "    ### cutmix ###\n",
    "    if cfg.cutmix:\n",
    "        config[\"cutmix_alpha\"] = tune.quniform(0.4, 1.0, 0.1)  \n",
    "        \n",
    "        hyperparam_mutations[\"cutmix_alpha\"] = [0.4, 1.0]\n",
    "\n",
    "    ######### Recipe 8 #########\n",
    "    ### weight decay tuning ###\n",
    "    if cfg.wd_tune:\n",
    "        config[\"weight_decay\"] = tune.qloguniform(1e-5, 1e-3, 1e-6)\n",
    "        \n",
    "        hyperparam_mutations[\"weight_decay\"] = [1e-5, 1e-3]\n",
    "\n",
    "    ######### Recipe 10 #########\n",
    "    ### model ema ###\n",
    "    if cfg.ema:\n",
    "        config[\"model_ema_steps\"] = tune.qrandint(15, 50, 5)\n",
    "        config[\"model_ema_decay\"] = tune.uniform(0.99, 0.99998)\n",
    "        \n",
    "        hyperparam_mutations[\"model_ema_steps\"] = [15, 50]\n",
    "        hyperparam_mutations[\"model_ema_decay\"] = [0.99, 0.99998]\n",
    "        \n",
    "    print(config)\n",
    "    load_data(config, data_dir)\n",
    "    \n",
    "    if cfg.asha:\n",
    "        scheduler = ASHAScheduler(\n",
    "            time_attr=\"training_iteration\",\n",
    "            metric=\"AUC\",\n",
    "            mode=\"max\",\n",
    "        )\n",
    "        \n",
    "    if cfg.ahb:\n",
    "        scheduler = AsyncHyperBandScheduler(\n",
    "            time_attr=\"training_iteration\",\n",
    "            metric=\"AUC\",\n",
    "            mode=\"max\",\n",
    "        )   \n",
    "    \n",
    "    if cfg.pbt:\n",
    "        scheduler = scheduler = PopulationBasedTraining(\n",
    "            time_attr=\"training_iteration\",\n",
    "            metric=\"AUC\",\n",
    "            mode=\"max\",\n",
    "            hyperparam_mutations=hyperparam_mutations\n",
    "        )\n",
    "        \n",
    "    if cfg.pb2:\n",
    "        scheduler = pb2.PB2(\n",
    "            time_attr=\"training_iteration\",\n",
    "            metric=\"AUC\",\n",
    "            mode=\"max\",\n",
    "            hyperparam_bounds=hyperparam_mutations\n",
    "        )\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"AUC\", \"training_iteration\"])\n",
    "        \n",
    "    result = tune.run(\n",
    "        partial(train, data_dir=data_dir),\n",
    "        name=cfg.name,\n",
    "        resources_per_trial={\"cpu\": cfg.cpus_per_trial, \"gpu\": cfg.gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=cfg.num_samples,\n",
    "        scheduler=scheduler,\n",
    "        stop={\"AUC\": 0.99},\n",
    "        resume=\"AUTO\",\n",
    "        progress_reporter=reporter\n",
    "    )\n",
    "\n",
    "    best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    ################ Run inference with best model #####################\n",
    "    _, _, test_dataset, num_classes = load_data(config, data_dir=data_dir)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "                                test_dataset,\n",
    "                                batch_size=32,\n",
    "                                sampler=torch.utils.data.SequentialSampler(test_dataset),\n",
    "                                num_workers=2,\n",
    "                                pin_memory=True)\n",
    "    \n",
    "    model = torch.hub.load(\"facebookresearch/deit:main\", \"deit_\"+cfg.model+\"_patch16_224\", pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    num_ftrs = model.head.in_features\n",
    "    model.head = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    ckpts = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "    model.load_state_dict(ckpts[\"model_state\"])\n",
    "    \n",
    "    \n",
    "    criterion = BinaryCrossEntropy(smoothing=0.0).to(device)\n",
    "    \n",
    "    test_auc, test_loss, task_aucs = valid_test_epoch(name='Test', \n",
    "                                                      epoch=None, \n",
    "                                                      model=model, \n",
    "                                                      device=device, \n",
    "                                                      data_loader=test_loader, \n",
    "                                                      criterion=criterion,\n",
    "                                                      limit=cfg.limit)\n",
    "\n",
    "    print(f\"Average AUC for all pathologies {test_auc:4.4f}\")\n",
    "    print(f\"Test loss: {test_loss:4.4f}\")                                 \n",
    "    print(f\"AUC for each task {[round(x, 4) for x in task_aucs]}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772321b-0806-4a38-a5c8-1e53ca4f70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser(add_help=True):\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Model Optimization Training\", add_help=add_help)\n",
    "    parser.add_argument(\"--dataset_dir\", default=\"../../../Music/work/data/\", type=str, help=\"dataset path\")\n",
    "    parser.add_argument(\"--dataset\", default=\"google\", type=str, help=\"dataset name\")\n",
    "    parser.add_argument(\"--seed\", default=99, type=int, help=\"\")\n",
    "    parser.add_argument(\"--model\", default=\"tiny\", type=str, help=\"\")\n",
    "    parser.add_argument(\"--num_samples\", default=5, type=int, help=\"\")\n",
    "    parser.add_argument(\"--epochs\", default=5, type=int, help=\"\")\n",
    "    parser.add_argument(\"--cpus_per_trial\", default=1, type=int, help=\"\")\n",
    "    parser.add_argument(\"--gpus_per_trial\", default=0, type=int, help=\"\")\n",
    "    parser.add_argument(\"--name\", default=\"hparams_tune\", type=str, help=\"\")\n",
    "    parser.add_argument(\"--limit\", default=None, type=int, help=\"\")\n",
    "    ###### scheduler type\n",
    "    parser.add_argument(\"--asha\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--ahb\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--pbt\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--pb2\", action=\"store_true\", default=False, help=\"\")\n",
    "    ###### pass to instantiate recipe                   \n",
    "    parser.add_argument(\"--lr_optim\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--trivial\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--random\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--random_erase\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--label_smoothing\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--mixup\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--cutmix\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--wd_tune\", action=\"store_true\", default=False, help=\"\")\n",
    "    parser.add_argument(\"--ema\", action=\"store_true\", default=False, help=\"\")\n",
    "    ###### pass to use optimized hparam                 \n",
    "    parser.add_argument(\"--batch_size\", default=None, type=int, help=\"\")\n",
    "    parser.add_argument(\"--lr\", default=None, type=float, help=\"\")\n",
    "    parser.add_argument(\"--lr_warmup_epochs\", default=None, type=int, help=\"\")\n",
    "    parser.add_argument(\"--lr_warmup_decay\", default=None, type=float, help=\"\")\n",
    "    parser.add_argument(\"--weight_decay\", default=None, type=float, help=\"\")\n",
    "    parser.add_argument(\"--smooth\", default=None, type=float, help=\"\")\n",
    "    parser.add_argument(\"--mixup_alpha\", default=None, type=float, help=\"\")\n",
    "    parser.add_argument(\"--cutmix_alpha\", default=None, type=float, help=\"\")\n",
    "    parser.add_argument(\"--random_erase_prob\", default=None, type=float, help=\"\")\n",
    "    parser.add_argument(\"--model_ema_steps\", default=None, type=int, help=\"\")\n",
    "    parser.add_argument(\"--model_ema_decay\", default=None, type=float, help=\"\")\n",
    "    parser.add_argument(\"--train_crop_size\", default=None, type=int, help=\"\")\n",
    "    parser.add_argument(\"--interpolation\", default=None, type=int, help=\"\")\n",
    "    parser.add_argument(\"--num_ops\", default=None, type=int, help=\"\")\n",
    "    parser.add_argument(\"--magnitude\", default=None, type=int, help=\"\")\n",
    "    parser.add_argument(\"--num_magnitude_bins\", default=None, type=int, help=\"\")\n",
    "    # parser.add_argument(\"--\", default=None, type=int, help=\"\")\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bfdea3-5973-4eef-9800-fcc8488d2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cfg = get_args_parser().parse_args()    \n",
    "    ### seed for ray[tune] schedulers\n",
    "    random.seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.random.manual_seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    result = main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4db96-180a-4436-b5bd-d301c82da0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
