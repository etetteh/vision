{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "id": "KNCxDvZg77HN",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iL3CiORx8Vm5",
    "outputId": "84b05fe3-20f1-49c4-87be-091c24b772fd"
   },
   "outputs": [],
   "source": [
    "! [ ! -z \"$COLAB_GPU\" ] && pip install torch torchvision ray timm\n",
    "! [ ! -z \"$COLAB_GPU\" ] && mkdir -p datasets\n",
    "! [ ! -z \"$COLAB_GPU\" ] && wget -nc --no-check-certificate https://download.pytorch.org/tutorial/hymenoptera_data.zip -P datasets\n",
    "! [ ! -z \"$COLAB_GPU\" ] && unzip -u datasets/hymenoptera_data.zip -d datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MtQRrA577HT"
   },
   "source": [
    "\n",
    "Transfer Learning for Computer Vision Tutorial\n",
    "==============================================\n",
    "**Author**: `Sasank Chilamkurthy <https://chsasank.github.io>`_\n",
    "\n",
    "In this tutorial, you will learn how to train a convolutional neural network for\n",
    "image classification using transfer learning. You can read more about the transfer\n",
    "learning at `cs231n notes <https://cs231n.github.io/transfer-learning/>`__\n",
    "\n",
    "Quoting these notes,\n",
    "\n",
    "    In practice, very few people train an entire Convolutional Network\n",
    "    from scratch (with random initialization), because it is relatively\n",
    "    rare to have a dataset of sufficient size. Instead, it is common to\n",
    "    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
    "    contains 1.2 million images with 1000 categories), and then use the\n",
    "    ConvNet either as an initialization or a fixed feature extractor for\n",
    "    the task of interest.\n",
    "\n",
    "These two major transfer learning scenarios look as follows:\n",
    "\n",
    "-  **Finetuning the convnet**: Instead of random initialization, we\n",
    "   initialize the network with a pretrained network, like the one that is\n",
    "   trained on imagenet 1000 dataset. Rest of the training looks as\n",
    "   usual.\n",
    "-  **ConvNet as fixed feature extractor**: Here, we will freeze the weights\n",
    "   for all of the network except that of the final fully connected\n",
    "   layer. This last fully connected layer is replaced with a new one\n",
    "   with random weights and only this layer is trained.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "ov0HOgV677HZ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7f96261dd160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import random_split\n",
    "import timm\n",
    "from functools import partial\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN9zENPv77Hb"
   },
   "source": [
    "Load Data\n",
    "---------\n",
    "\n",
    "We will use torchvision and torch.utils.data packages for loading the\n",
    "data.\n",
    "\n",
    "The problem we're going to solve today is to train a model to classify\n",
    "**ants** and **bees**. We have about 120 training images each for ants and bees.\n",
    "There are 75 validation images for each class. Usually, this is a very\n",
    "small dataset to generalize upon, if trained from scratch. Since we\n",
    "are using transfer learning, we should be able to generalize reasonably\n",
    "well.\n",
    "\n",
    "This dataset is a very small subset of imagenet.\n",
    "\n",
    ".. Note ::\n",
    "   Download the data from\n",
    "   `here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_\n",
    "   and extract it to the current directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1xbnPtQ8BKx",
    "outputId": "c7eb059e-f7e8-4806-b6b5-d8125514ee98"
   },
   "outputs": [],
   "source": [
    "# def download_and_extract_data(dataset_dir='datasets'):\n",
    "#     data_zip = os.path.join(dataset_dir, 'hymenoptera_data.zip')\n",
    "#     data_path = os.path.join(dataset_dir, 'hymenoptera_data')\n",
    "#     url = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
    "\n",
    "#     if not os.path.exists(data_path):\n",
    "#         if not os.path.exists(data_zip):\n",
    "#             print(\"Starting to download data...\")\n",
    "#             data = request.urlopen(url, timeout=15).read()\n",
    "#             with open(data_zip, 'wb') as f:\n",
    "#                 f.write(data)\n",
    "\n",
    "#         print(\"Starting to extract data...\")\n",
    "#         with ZipFile(data_zip, 'r') as zip_f:\n",
    "#             zip_f.extractall(dataset_dir)\n",
    "        \n",
    "#     print(\"Data has been downloaded and extracted to {}.\".format(dataset_dir))\n",
    "    \n",
    "# download_and_extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "3DsscrSR77Hd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "\n",
    "\n",
    "def load_data(data_dir='./hymenoptera_data'):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    trainset = datasets.ImageFolder(os.path.join(data_dir, 'train'),\n",
    "                                              data_transforms['train'])\n",
    "\n",
    "    testset = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n",
    "                                              data_transforms['val'])\n",
    "\n",
    "    return trainset, testset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HOvIWaTP77He"
   },
   "outputs": [],
   "source": [
    "def train(config, checkpoint_dir=None, data_dir=None):\n",
    "    seed = 99\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    model = torch.hub.load(\"facebookresearch/deit:main\", \"deit_tiny_patch16_224\", pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    num_ftrs = model.head.in_features\n",
    "    model.head = nn.Linear(num_ftrs, 2)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.head.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "    \n",
    "#     main_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#             optimizer, T_max=max_num_epochs - 6\n",
    "#         )\n",
    "\n",
    "#     warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#                     optimizer, start_factor=0.0001, total_iters=6\n",
    "#                 )\n",
    "\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "#                 optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[6]\n",
    "#             )  \n",
    "\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        checkpoints = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model_state, optimizer_state = checkpoints[\"model\"], checkpoints[\"optimizer\"]\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "    \n",
    "    ##### Data loaders\n",
    "    trainset, testset = load_data(data_dir)\n",
    "    \n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "\n",
    "    \n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "                \n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save({\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()}, path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "0eVRARf377Hg",
    "outputId": "afffa101-2e54-4227-b2fe-260429b56dab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 23:07:17,601\tWARNING experiment.py:255 -- No name detected on trainable. Using DEFAULT.\n",
      "2021-12-07 23:07:17,612\tINFO registry.py:69 -- Detected unknown callable for trainable. Converting to class.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ray.cloudpickle.dumps(<class 'ray.tune.function_runner.wrap_function.<locals>.ImplicitFunc'>) failed.\nTo check which non-serializable variables are captured in scope, re-run the ray script with 'RAY_PICKLE_VERBOSE_DEBUG=1'. Other options: \n-Try reproducing the issue by calling `pickle.dumps(trainable)`. \n-If the error is typing-related, try removing the type annotations and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9048/1824003332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# You can change the number of GPUs per trial here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_9048/1824003332.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n\u001b[0;32m---> 17\u001b[0;31m     result = tune.run(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mresources_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             experiments[i] = Experiment(\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/ray/tune/experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, run, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, sync_config, trial_name_creator, trial_dirname_creator, log_to_file, checkpoint_freq, checkpoint_at_end, keep_checkpoints_num, checkpoint_score_attr, export_formats, max_failures, restore)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;34m\"checkpointable function. You can specify checkpoints \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \"within your trainable function.\")\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_identifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_identifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/ray/tune/experiment.py\u001b[0m in \u001b[0;36mregister_if_needed\u001b[0;34m(cls, run_object)\u001b[0m\n\u001b[1;32m    263\u001b[0m                              \u001b[0;34m\"\\n-If the error is typing-related, try removing \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                              \"the type annotations and try again.\")\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ray.cloudpickle.dumps(<class 'ray.tune.function_runner.wrap_function.<locals>.ImplicitFunc'>) failed.\nTo check which non-serializable variables are captured in scope, re-run the ray script with 'RAY_PICKLE_VERBOSE_DEBUG=1'. Other options: \n-Try reproducing the issue by calling `pickle.dumps(trainable)`. \n-If the error is typing-related, try removing the type annotations and try again."
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    data_dir = os.path.abspath(\"./hymenoptera_data\")\n",
    "    load_data(data_dir)\n",
    "    config = {\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        partial(train, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "#     best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "#     device = \"cpu\"\n",
    "#     if torch.cuda.is_available():\n",
    "#         device = \"cuda:0\"\n",
    "#         if gpus_per_trial > 1:\n",
    "#             best_trained_model = nn.DataParallel(best_trained_model)\n",
    "#     best_trained_model.to(device)\n",
    "\n",
    "#     best_checkpoint_dir = best_trial.checkpoint.value\n",
    "#     model_state, optimizer_state = torch.load(os.path.join(\n",
    "#         best_checkpoint_dir, \"checkpoint\"))\n",
    "#     best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "#     test_acc = test_accuracy(best_trained_model, device)\n",
    "#     print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1Llq0Qp77Hi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhXC_cH777Hj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "070eLkQC77Hj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZbsPzdU77Hj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "asha.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
