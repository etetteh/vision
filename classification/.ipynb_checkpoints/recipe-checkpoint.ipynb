{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import timm\n",
    "import torch, torchvision\n",
    "\n",
    "from torch import nn\n",
    "from timm.loss import BinaryCrossEntropy, LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import autoaugment, transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "import transforms as T\n",
    "import utils\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining, pb2\n",
    "\n",
    "\n",
    "import simpleargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ee014d950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 99\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir='./hymenoptera_data'):\n",
    "    augment = [transforms.RandomResizedCrop(size=224, interpolation=InterpolationMode.BILINEAR),\n",
    "               transforms.RandomHorizontalFlip()]\n",
    "    ######### Recipe 2 #########           \n",
    "    if cfg.num_ops:\n",
    "        augment.append(autoaugment.RandAugment(num_ops=cfg.num_ops, magnitude=cfg.magnitude, num_magnitude_bins=cfg.num_magnitude_bins, interpolation=cfg.interpolation))\n",
    "        \n",
    "    if cfg.num_magnitude_bins:\n",
    "        augment.append(autoaugment.TrivialAugmentWide(num_magnitude_bins=cfg.num_magnitude_bins, interpolation=cfg.interpolation))\n",
    "\n",
    "    augment.extend([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),])  \n",
    "    ######### Recipe 4 #########\n",
    "    if cfg.random_erase_prob:\n",
    "        augment.append(transforms.RandomErasing(p=cfg.random_erase_prob))\n",
    "    \n",
    "    ######### Recipe 9 #########\n",
    "    if cfg.train_crop_size:\n",
    "        augment[0] = transforms.RandomResizedCrop(size=cfg.train_crop_size, interpolation=cfg.interpolate)  \n",
    "\n",
    "    augment = transforms.Compose(augment)\n",
    "    \n",
    "    valid_augment = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "                       \n",
    "    trainset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=augment)\n",
    "    validset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=valid_augment)\n",
    "    num_classes = len(trainset.classes)\n",
    "    return trainset, validset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(checkpoint_dir=None, data_dir=None):\n",
    "    seed = 99\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    trainset, validset, num_classes = load_data(data_dir=data_dir)\n",
    "    \n",
    "    model = torch.hub.load(\"facebookresearch/deit:main\", \"deit_\"+cfg.model+\"_patch16_224\", pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    num_ftrs = model.head.in_features\n",
    "    model.head = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    ######### Recipe 10 #########\n",
    "    ema_model = None\n",
    "    if (cfg.model_ema_decay and cfg.model_ema_steps):\n",
    "        adjust = 1 * cfg.batch_size * cfg.model_ema_steps / cfg.epochs\n",
    "        alpha = 1.0 - cfg.model_ema_decay\n",
    "        alpha = min(1.0, alpha * adjust)\n",
    "        ema_model = utils.ExponentialMovingAverage(model, device=device, decay=1.0 - alpha)\n",
    "        \n",
    "    \n",
    "    parameters = model.head.parameters()\n",
    "    \n",
    "    ######### Recipe 8 #########                   \n",
    "    if cfg.weight_decay:\n",
    "        norm_weight_decay=0.0\n",
    "        param_groups = utils.split_normalization_params(model)\n",
    "        wd_groups = [norm_weight_decay, cfg.weight_decay]\n",
    "        parameters = [{\"params\": p, \"weight_decay\": w} for p, w in zip(param_groups, wd_groups) if p]\n",
    "    \n",
    "    \n",
    "    ######### Recipe 1 #########\n",
    "    if cfg.lr:\n",
    "        optimizer = torch.optim.AdamW(parameters, lr=cfg.lr)\n",
    "        main_lr_scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs - cfg.lr_warmup_epochs)\n",
    "        warmup_lr_scheduler = LinearLR(optimizer, start_factor=cfg.lr_warmup_decay, total_iters=cfg.lr_warmup_epochs)\n",
    "        lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[cfg.lr_warmup_epochs])\n",
    "        \n",
    "    ######### Recipe 5 #########\n",
    "    if cfg.mixup_alpha or cfg.cutmix_alpha:\n",
    "        if cfg.bce:\n",
    "            train_criterion = BinaryCrossEntropy(smoothing=0.0)\n",
    "        else:\n",
    "            train_criterion = SoftTargetCrossEntropy()\n",
    "    elif cfg.smooth:\n",
    "        if cfg.bce:\n",
    "            train_criterion = BinaryCrossEntropy(smoothing=cfg.smooth)\n",
    "        else:\n",
    "            train_criterion = LabelSmoothingCrossEntropy(smoothing=cfg.smooth)\n",
    "    else:\n",
    "        train_criterion = nn.CrossEntropyLoss()\n",
    "    train_criterion = train_criterion.to(device)\n",
    "\n",
    "\n",
    "    valid_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    collate_fn = None\n",
    "    mixup_transforms = []\n",
    "    ######### Recipe 6 #########\n",
    "    if cfg.mixup_alpha:\n",
    "        mixup_transforms.append(T.RandomMixup(num_classes, p=1.0, alpha=mixup_alpha))\n",
    "    ######### Recipe 7 #########    \n",
    "    if cfg.cutmix_alpha:\n",
    "        mixup_transforms.append(T.RandomCutmix(num_classes, p=1.0, alpha=cfg.cutmix_alpha))\n",
    "    if mixup_transforms:\n",
    "        mixupcutmix = torchvision.transforms.RandomChoice(mixup_transforms)\n",
    "        collate_fn = lambda batch: mixupcutmix(*default_collate(batch))\n",
    "        \n",
    "        \n",
    "    if checkpoint_dir:\n",
    "        checkpoints = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(checkpoints[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoints[\"optimizer_state\"])\n",
    "        lr_scheduler.load_state_dict(checkpoints[\"lr_scheduler_state\"])\n",
    "        if ema_model:\n",
    "            ema_model.load_state_dict(checkpoints[\"model_ema\"])\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        validset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True)\n",
    "    \n",
    "    \n",
    "    best=0.0\n",
    "    for epoch in range(cfg.epochs): \n",
    "        train_one_epoch(epoch, train_loader, model, ema_model, optimizer, train_criterion, device=device)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        if ema_model:\n",
    "            valid_loss, accuracy = validate(valid_loader, ema_model, valid_criterion, device=device)\n",
    "        else:\n",
    "            valid_loss, accuracy = validate(valid_loader, model, valid_criterion, device=device)\n",
    "        \n",
    "          \n",
    "            \n",
    "        print(f\"Epoch: {epoch} Valid loss:{valid_loss:4.4f} Valid accuracy: {accuracy:4.4f}\")\n",
    "        \n",
    "        if checkpoint_dir is not None and not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir+\"/checkpoint\")\n",
    "        if checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            ckpts = {\"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict(), \"lr_scheduler_state\": lr_scheduler.state_dict()}\n",
    "            if ema_model:\n",
    "                ckpts[\"ema_state\"] = ema_model.state_dict()\n",
    "            torch.save(ckpts, path)\n",
    "            \n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, train_loader, model, ema_model, optimizer, criterion, device=\"cpu\"):\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / epoch_steps))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        if ema_model and i % cfg.model_ema_steps == 0:\n",
    "            ema_model.update_parameters(model)\n",
    "            if epoch < cfg.lr_warmup_epochs:\n",
    "                # Reset ema buffer to keep copying weights during warmup period\n",
    "                ema_model.n_averaged.fill_(0)\n",
    "\n",
    "\n",
    "def validate(valid_loader, model, criterion, device=\"cpu\"):\n",
    "    valid_loss = 0.0\n",
    "    valid_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.cpu().numpy()\n",
    "            valid_steps += 1\n",
    "\n",
    "    valid_loss = valid_loss / valid_steps\n",
    "    accuracy = correct / total\n",
    "    return  valid_loss, accuracy\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = simpleargs\n",
    "data_dir=\"./hymenoptera_data/\"\n",
    "# cfg.seed=99\n",
    "cfg.model=\"tiny\"\n",
    "cfg.epochs=25\n",
    "###### scheduler type\n",
    "cfg.bce=True\n",
    "###### pass to use optimized hparam  \n",
    "cfg.batch_size=64\n",
    "cfg.lr=0.00491\n",
    "cfg.lr_warmup_epochs=6\n",
    "cfg.lr_warmup_decay=3.7999999e-05\n",
    "cfg.weight_decay=None\n",
    "cfg.smooth=None\n",
    "cfg.mixup_alpha=None\n",
    "cfg.cutmix_alpha=None\n",
    "cfg.random_erase_prob=None\n",
    "cfg.model_ema_steps=None\n",
    "cfg.model_ema_decay=None\n",
    "cfg.train_crop_size=None\n",
    "cfg.interpolation=None\n",
    "cfg.num_ops=None\n",
    "cfg.magnitude=None\n",
    "cfg.num_magnitude_bins=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/enoch/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Valid loss:0.6928 Valid accuracy: 0.6340\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25250/697005498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_25250/3725119319.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(checkpoint_dir, data_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mbest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mema_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25250/2504694828.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch, train_loader, model, ema_model, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepoch_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "train(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
